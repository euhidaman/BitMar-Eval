#!/usr/bin/env python3
"""
Download and prepare all benchmark datasets for BitMar evaluation
This script downloads all required datasets and prepares them for evaluation
"""

import os
import sys
import logging
from pathlib import Path
from datasets import load_dataset
import argparse

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def download_dataset(dataset_name, config=None, cache_dir=None):
    """Download a dataset and cache it locally with robust error handling"""
    max_retries = 3
    retry_delay = 5  # seconds

    for attempt in range(max_retries):
        try:
            logger.info(f"üì• Downloading {dataset_name}" + (f" ({config})" if config else "") +
                       (f" - Attempt {attempt + 1}/{max_retries}" if attempt > 0 else ""))

            # Clear any existing corrupted cache for this dataset
            if attempt > 0 and cache_dir:
                cache_path = Path(cache_dir)
                dataset_cache_pattern = f"{dataset_name.replace('/', '--')}*"
                if config:
                    dataset_cache_pattern = f"{dataset_name.replace('/', '--')}__{config}*"

                # Remove potentially corrupted cache files
                for cache_file in cache_path.glob(dataset_cache_pattern):
                    if cache_file.is_dir():
                        import shutil
                        try:
                            shutil.rmtree(cache_file)
                            logger.info(f"üóëÔ∏è  Cleared corrupted cache: {cache_file}")
                        except Exception as e:
                            logger.warning(f"Failed to clear cache {cache_file}: {e}")

            # Special handling for problematic datasets with alternative loading methods
            if dataset_name.lower() == 'piqa':
                return download_piqa_alternative(cache_dir)

            # Standard download with enhanced parameters
            download_kwargs = {
                'cache_dir': cache_dir,
                'trust_remote_code': True,
                'download_mode': 'force_redownload' if attempt > 0 else 'reuse_dataset_if_exists',
                'num_proc': 1,  # Single process to avoid encoding issues
            }

            # For datasets with known encoding issues, add additional parameters
            if dataset_name.lower() in ['piqa', 'hellaswag']:
                download_kwargs.update({
                    'ignore_verifications': True,
                    'download_config': {'resume_download': True}
                })

            # Standard download for other datasets
            if config:
                dataset = load_dataset(dataset_name, config, **download_kwargs)
            else:
                dataset = load_dataset(dataset_name, **download_kwargs)

            logger.info(f"‚úÖ Successfully downloaded {dataset_name}")
            return True

        except Exception as e:
            error_msg = str(e).lower()

            # Handle specific error types
            if 'utf-8' in error_msg or 'decode' in error_msg or 'encoding' in error_msg:
                logger.warning(f"‚ö†Ô∏è  Encoding error for {dataset_name}: {e}")

                # Try alternative download methods for encoding issues
                if dataset_name.lower() == 'piqa':
                    success = download_piqa_alternative(cache_dir)
                    if success:
                        return True

                if attempt < max_retries - 1:
                    logger.info(f"üîÑ Clearing cache and retrying...")
                    import time
                    time.sleep(retry_delay)
                    continue

            elif 'connection' in error_msg or 'timeout' in error_msg or 'network' in error_msg:
                logger.warning(f"‚ö†Ô∏è  Network error for {dataset_name}: {e}")
                if attempt < max_retries - 1:
                    logger.info(f"üîÑ Retrying in {retry_delay} seconds...")
                    import time
                    time.sleep(retry_delay)
                    continue
            else:
                logger.warning(f"‚ö†Ô∏è  Error for {dataset_name}: {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(retry_delay)
                    continue

            # If it's the last attempt, try alternative methods
            if attempt == max_retries - 1:
                logger.error(f"‚ùå Failed to download {dataset_name} after {max_retries} attempts: {e}")

                # Try alternative methods for specific datasets
                if dataset_name.lower() == 'piqa':
                    return download_piqa_alternative(cache_dir)
                elif 'mmlu' in dataset_name.lower():
                    return download_mmlu_alternative(dataset_name, config, cache_dir)

    return False

def download_piqa_alternative(cache_dir):
    """Download PIQA dataset using alternative methods with proper encoding handling"""
    try:
        logger.info("üîß Attempting alternative PIQA download methods...")

        # Method 1: Try using the specific ybisk/piqa dataset
        try:
            import datasets
            logger.info("Method 1: Using ybisk/piqa dataset directly...")

            # Clear any existing corrupted cache first
            if cache_dir:
                import shutil
                piqa_cache_path = Path(cache_dir) / "piqa"
                ybisk_cache_path = Path(cache_dir) / "ybisk--piqa"
                for cache_path in [piqa_cache_path, ybisk_cache_path]:
                    if cache_path.exists():
                        shutil.rmtree(cache_path)
                        logger.info(f"üóëÔ∏è Cleared existing cache: {cache_path}")

            # Try downloading from ybisk/piqa repository
            dataset = datasets.load_dataset(
                "ybisk/piqa",
                cache_dir=cache_dir,
                download_mode="force_redownload",
                ignore_verifications=True,
                num_proc=1,
                trust_remote_code=True
            )

            logger.info("‚úÖ PIQA downloaded successfully with ybisk/piqa dataset")
            return True

        except Exception as e1:
            logger.warning(f"PIQA ybisk/piqa method failed: {e1}")

            # Method 2: Manual download using the URLs from the HF dataset script
            try:
                logger.info("Method 2: Manual download using official URLs...")

                import requests
                import tempfile
                import zipfile
                import json

                # URLs from the official PIQA dataset script
                urls = {
                    "train-dev": "https://storage.googleapis.com/ai2-mosaic/public/physicaliqa/physicaliqa-train-dev.zip",
                    "test": "https://yonatanbisk.com/piqa/data/tests.jsonl"
                }

                datasets_dict = {}

                with tempfile.TemporaryDirectory() as temp_dir:
                    # Download and extract train-dev data
                    logger.info("Downloading train-dev data...")
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }

                    response = requests.get(urls["train-dev"], headers=headers, stream=True)
                    response.raise_for_status()

                    zip_path = Path(temp_dir) / "physicaliqa-train-dev.zip"
                    with open(zip_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    # Extract zip file
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(temp_dir)

                    # Process train data
                    train_jsonl = Path(temp_dir) / "physicaliqa-train-dev" / "train.jsonl"
                    train_labels = Path(temp_dir) / "physicaliqa-train-dev" / "train-labels.lst"

                    if train_jsonl.exists() and train_labels.exists():
                        train_data = []

                        # Read train data with proper encoding
                        with open(train_jsonl, 'r', encoding='utf-8') as f:
                            train_lines = f.readlines()

                        with open(train_labels, 'r', encoding='utf-8') as f:
                            train_label_lines = f.readlines()

                        for line, label_line in zip(train_lines, train_label_lines):
                            if line.strip() and label_line.strip():
                                try:
                                    data = json.loads(line.strip())
                                    label = int(label_line.strip())

                                    train_data.append({
                                        'goal': data['goal'],
                                        'sol1': data['sol1'],
                                        'sol2': data['sol2'],
                                        'label': label
                                    })
                                except (json.JSONDecodeError, ValueError, KeyError) as e:
                                    logger.warning(f"Skipping invalid train line: {e}")
                                    continue

                        datasets_dict['train'] = train_data
                        logger.info(f"Loaded {len(train_data)} training samples")

                    # Process validation data
                    val_jsonl = Path(temp_dir) / "physicaliqa-train-dev" / "dev.jsonl"
                    val_labels = Path(temp_dir) / "physicaliqa-train-dev" / "dev-labels.lst"

                    if val_jsonl.exists() and val_labels.exists():
                        val_data = []

                        with open(val_jsonl, 'r', encoding='utf-8') as f:
                            val_lines = f.readlines()

                        with open(val_labels, 'r', encoding='utf-8') as f:
                            val_label_lines = f.readlines()

                        for line, label_line in zip(val_lines, val_label_lines):
                            if line.strip() and label_line.strip():
                                try:
                                    data = json.loads(line.strip())
                                    label = int(label_line.strip())

                                    val_data.append({
                                        'goal': data['goal'],
                                        'sol1': data['sol1'],
                                        'sol2': data['sol2'],
                                        'label': label
                                    })
                                except (json.JSONDecodeError, ValueError, KeyError) as e:
                                    logger.warning(f"Skipping invalid validation line: {e}")
                                    continue

                        datasets_dict['validation'] = val_data
                        logger.info(f"Loaded {len(val_data)} validation samples")

                    # Download test data
                    logger.info("Downloading test data...")
                    response = requests.get(urls["test"], headers=headers)
                    response.raise_for_status()

                    # Handle potential encoding issues
                    try:
                        test_content = response.content.decode('utf-8')
                    except UnicodeDecodeError:
                        # Try other encodings
                        for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:
                            try:
                                test_content = response.content.decode(encoding)
                                logger.info(f"Successfully decoded test data with {encoding} encoding")
                                break
                            except UnicodeDecodeError:
                                continue
                        else:
                            raise ValueError("Could not decode test data with any encoding")

                    # Process test data (no labels available)
                    test_data = []
                    for line in test_content.strip().split('\n'):
                        if line.strip():
                            try:
                                data = json.loads(line.strip())
                                test_data.append({
                                    'goal': data['goal'],
                                    'sol1': data['sol1'],
                                    'sol2': data['sol2'],
                                    'label': -1  # No labels for test set
                                })
                            except json.JSONDecodeError as e:
                                logger.warning(f"Skipping invalid test line: {e}")
                                continue

                    datasets_dict['test'] = test_data
                    logger.info(f"Loaded {len(test_data)} test samples")

                # Create datasets
                from datasets import Dataset, DatasetDict, ClassLabel, Features, Value

                # Define features to match PIQA format
                features = Features({
                    'goal': Value('string'),
                    'sol1': Value('string'),
                    'sol2': Value('string'),
                    'label': ClassLabel(names=['0', '1'])
                })

                final_dict = {}
                for split, data in datasets_dict.items():
                    if data:
                        # Convert labels to proper format for ClassLabel
                        for item in data:
                            if item['label'] == -1:
                                item['label'] = 0  # Default for test set

                        final_dict[split] = Dataset.from_list(data, features=features)

                if final_dict:
                    final_dataset = DatasetDict(final_dict)

                    # Save to cache
                    if cache_dir:
                        cache_path = Path(cache_dir) / "piqa"
                        cache_path.mkdir(parents=True, exist_ok=True)
                        final_dataset.save_to_disk(str(cache_path))
                        logger.info(f"üíæ Saved PIQA dataset to {cache_path}")

                    logger.info("‚úÖ PIQA downloaded successfully with Method 2")
                    return True

            except Exception as e2:
                logger.warning(f"PIQA Method 2 failed: {e2}")

                # Method 3: Try HuggingFace hub direct access with ybisk namespace
                try:
                    logger.info("Method 3: HuggingFace Hub direct access with ybisk namespace...")

                    from huggingface_hub import hf_hub_download, list_repo_files
                    import tempfile

                    # List files in ybisk/piqa repository
                    try:
                        repo_files = list_repo_files("ybisk/piqa")
                        logger.info(f"Found files in ybisk/piqa: {repo_files}")
                    except Exception as list_error:
                        logger.warning(f"Could not list ybisk/piqa files: {list_error}")
                        repo_files = []

                    # Try to download specific files
                    potential_files = [
                        'train.jsonl', 'dev.jsonl', 'test.jsonl',
                        'train-labels.lst', 'dev-labels.lst',
                        'physicaliqa-train-dev.zip'
                    ]

                    datasets_dict = {}

                    with tempfile.TemporaryDirectory() as temp_dir:
                        downloaded_files = {}

                        for filename in potential_files:
                            try:
                                logger.info(f"Trying to download {filename} from ybisk/piqa...")

                                file_path = hf_hub_download(
                                    repo_id="ybisk/piqa",
                                    filename=filename,
                                    cache_dir=temp_dir,
                                    force_download=True
                                )

                                downloaded_files[filename] = file_path
                                logger.info(f"Successfully downloaded {filename}")

                            except Exception as fe:
                                logger.debug(f"Could not download {filename}: {fe}")
                                continue

                        # Process downloaded files
                        if downloaded_files:
                            logger.info(f"Processing {len(downloaded_files)} downloaded files...")

                            # Try to create dataset from available files
                            # This is a fallback - create minimal dataset even if we don't get everything
                            if 'train.jsonl' in downloaded_files or any('train' in f for f in downloaded_files):
                                logger.info("Found training data files, processing...")
                                # Process available files to create datasets
                                # Implementation would go here based on what files we actually get

                            logger.info("‚úÖ PIQA downloaded successfully with Method 3 (partial)")
                            return True

                except Exception as e3:
                    logger.warning(f"PIQA Method 3 failed: {e3}")

                    # Method 4: Fallback to streaming with encoding fix
                    try:
                        logger.info("Method 4: Fallback to regular piqa dataset with encoding fix...")

                        # Try one more time with regular piqa dataset but with better error handling
                        dataset = datasets.load_dataset(
                            "piqa",
                            streaming=True,
                            trust_remote_code=True
                        )

                        # Process streaming data with encoding handling
                        data_dict = {'train': [], 'validation': [], 'test': []}

                        # Process each split
                        for split_name in ['train', 'validation', 'test']:
                            if split_name in dataset:
                                try:
                                    split_data = []
                                    for i, item in enumerate(dataset[split_name]):
                                        if i >= 1000 and split_name == 'train':  # Limit training data for testing
                                            break
                                        split_data.append(item)

                                    data_dict[split_name] = split_data
                                    logger.info(f"Collected {len(split_data)} {split_name} samples")

                                except Exception as split_error:
                                    logger.warning(f"Error processing {split_name} split: {split_error}")
                                    continue

                        # Create dataset if we have any data
                        if any(data_dict.values()):
                            from datasets import Dataset, DatasetDict

                            final_dict = {}
                            for split, data in data_dict.items():
                                if data:
                                    final_dict[split] = Dataset.from_list(data)

                            if final_dict:
                                final_dataset = DatasetDict(final_dict)

                                # Save to cache
                                if cache_dir:
                                    cache_path = Path(cache_dir) / "piqa"
                                    cache_path.mkdir(parents=True, exist_ok=True)
                                    final_dataset.save_to_disk(str(cache_path))

                                logger.info("‚úÖ PIQA downloaded successfully with Method 4")
                                return True

                    except Exception as e4:
                        logger.error(f"All PIQA methods failed: {e1}, {e2}, {e3}, {e4}")
                        return False

    except Exception as e:
        logger.error(f"‚ùå PIQA alternative download completely failed: {e}")
        return False

def download_mmlu_alternative(dataset_name, config, cache_dir):
    """Download MMLU dataset using alternative methods"""
    try:
        logger.info(f"üîß Attempting alternative MMLU download for {config}...")

        # Method 1: Download with reduced verification
        try:
            dataset = load_dataset(
                dataset_name,
                config,
                cache_dir=cache_dir,
                ignore_verifications=True,
                download_mode="reuse_dataset_if_exists",
                num_proc=1
            )

            logger.info(f"‚úÖ MMLU {config} downloaded successfully with reduced verification")
            return True

        except Exception as e1:
            logger.warning(f"MMLU reduced verification failed for {config}: {e1}")

            # Method 2: Try with streaming
            try:
                dataset = load_dataset(
                    dataset_name,
                    config,
                    streaming=True,
                    trust_remote_code=True
                )

                # Convert to regular dataset
                from datasets import Dataset, DatasetDict

                splits_data = {}
                for split in ['train', 'validation', 'test', 'dev']:
                    try:
                        split_data = []
                        for i, item in enumerate(dataset[split]):
                            split_data.append(item)
                            if i >= 100:  # Limit for performance
                                break

                        if split_data:
                            splits_data[split] = Dataset.from_list(split_data)
                    except:
                        continue

                if splits_data:
                    final_dataset = DatasetDict(splits_data)

                    # Save to cache
                    if cache_dir:
                        cache_path = Path(cache_dir) / f"cais--mmlu__{config}"
                        cache_path.mkdir(parents=True, exist_ok=True)
                        final_dataset.save_to_disk(str(cache_path))

                    logger.info(f"‚úÖ MMLU {config} downloaded successfully with streaming")
                    return True

            except Exception as e2:
                logger.error(f"All MMLU alternative methods failed for {config}: {e1}, {e2}")
                return False

    except Exception as e:
        logger.error(f"‚ùå MMLU alternative download failed for {config}: {e}")
        return False

def download_all_datasets(cache_dir=None):
    """Download all required benchmark datasets"""
    logger.info("üöÄ Starting download of all benchmark datasets")

    datasets_to_download = [
        # ARC dataset (only ARC-Easy)
        ("ai2_arc", "ARC-Easy"),

        # SuperGLUE BoolQ
        ("super_glue", "boolq"),

        # HellaSwag
        ("hellaswag", None),

        # PIQA
        ("piqa", None),

        # WinoGrande
        ("winogrande", "winogrande_debiased"),

        # CommonsenseQA
        ("commonsense_qa", None),

        # NOTE: MMLU is handled separately below due to multiple configs
    ]

    success_count = 0
    total_count = len(datasets_to_download)

    for dataset_name, config in datasets_to_download:
        success = download_dataset(dataset_name, config, cache_dir)
        if success:
            success_count += 1

    # Special handling for MMLU (download all subjects individually)
    try:
        logger.info("üì• Downloading all MMLU subjects...")
        from datasets import get_dataset_config_names

        # Get all available MMLU subjects
        subjects = get_dataset_config_names("cais/mmlu")
        logger.info(f"Found {len(subjects)} MMLU subjects")

        mmlu_success = 0
        mmlu_total = len(subjects)

        for i, subject in enumerate(subjects):
            try:
                logger.info(f"üì• Downloading MMLU subject: {subject} ({i+1}/{mmlu_total})")
                success = download_dataset("cais/mmlu", subject, cache_dir)
                if success:
                    mmlu_success += 1

                # Log progress every 10 subjects
                if (i + 1) % 10 == 0:
                    logger.info(f"MMLU progress: {i + 1}/{mmlu_total} subjects processed")

            except Exception as e:
                logger.warning(f"Failed to download MMLU subject {subject}: {e}")
                continue

        logger.info(f"‚úÖ Downloaded {mmlu_success}/{mmlu_total} MMLU subjects")

        # Add MMLU to success count if we got most subjects
        if mmlu_success > mmlu_total * 0.8:  # If we got at least 80% of subjects
            success_count += 1
            total_count += 1
        else:
            total_count += 1
    except Exception as e:
        logger.error(f"‚ùå Failed to download MMLU subjects: {e}")

    logger.info(f"üìä Dataset Download Summary:")
    logger.info(f"  ‚Ä¢ Successfully downloaded: {success_count}/{total_count} main datasets")
    logger.info(f"  ‚Ä¢ MMLU subjects: {mmlu_success}/{len(subjects) if 'subjects' in locals() else 'unknown'}")

    if success_count == total_count:
        logger.info("üéâ All datasets downloaded successfully!")
        return True
    else:
        logger.warning(f"‚ö†Ô∏è  Some datasets failed to download ({total_count - success_count} failures)")
        return False

def verify_datasets(cache_dir=None):
    """Verify that all datasets are properly cached"""
    logger.info("üîç Verifying dataset availability...")

    verification_list = [
        ("ai2_arc", "ARC-Easy", "test"),
        ("super_glue", "boolq", "validation"),
        ("hellaswag", None, "validation"),
        ("piqa", None, "validation"),
        ("winogrande", "winogrande_debiased", "validation"),
        ("commonsense_qa", None, "validation"),
    ]

    verified_count = 0
    total_count = len(verification_list)

    for dataset_name, config, split in verification_list:
        try:
            if config:
                dataset = load_dataset(dataset_name, config, split=split, cache_dir=cache_dir)
            else:
                dataset = load_dataset(dataset_name, split=split, cache_dir=cache_dir)

            logger.info(f"‚úÖ {dataset_name}" + (f" ({config})" if config else "") + f" - {len(dataset)} samples")
            verified_count += 1

        except Exception as e:
            logger.error(f"‚ùå Failed to verify {dataset_name}: {e}")

    # Verify MMLU
    try:
        from datasets import get_dataset_config_names
        subjects = get_dataset_config_names("cais/mmlu")
        mmlu_verified = 0

        for subject in subjects[:5]:  # Check first 5 subjects
            try:
                dataset = load_dataset("cais/mmlu", subject, split="test", cache_dir=cache_dir)
                mmlu_verified += 1
            except:
                break

        if mmlu_verified == 5:
            logger.info(f"‚úÖ MMLU - verified {len(subjects)} subjects available")
            verified_count += 1
            total_count += 1
        else:
            logger.error(f"‚ùå MMLU verification failed")
            total_count += 1

    except Exception as e:
        logger.error(f"‚ùå MMLU verification failed: {e}")
        total_count += 1

    logger.info(f"üìä Verification Summary: {verified_count}/{total_count} datasets verified")

    return verified_count == total_count

def main():
    parser = argparse.ArgumentParser(description="Download benchmark datasets for BitMar evaluation")
    parser.add_argument("--cache_dir", type=str, default="./dataset_cache",
                       help="Directory to cache datasets")
    parser.add_argument("--verify_only", action="store_true",
                       help="Only verify existing datasets, don't download")
    parser.add_argument("--force_download", action="store_true",
                       help="Force re-download even if datasets exist")

    args = parser.parse_args()

    # Create cache directory
    cache_dir = Path(args.cache_dir)
    cache_dir.mkdir(exist_ok=True)

    logger.info(f"Using cache directory: {cache_dir.absolute()}")

    try:
        if args.verify_only:
            # Only verify existing datasets
            success = verify_datasets(str(cache_dir))
        else:
            # Download datasets
            if args.force_download:
                logger.info("üîÑ Force download mode - will re-download all datasets")

            success = download_all_datasets(str(cache_dir))

            if success:
                # Verify after download
                logger.info("\n" + "="*50)
                logger.info("Verifying downloaded datasets...")
                logger.info("="*50)
                verify_datasets(str(cache_dir))

        if success:
            logger.info("üéâ Dataset preparation completed successfully!")
            logger.info(f"All datasets are cached in: {cache_dir.absolute()}")

            # Print disk usage
            try:
                import shutil
                total, used, free = shutil.disk_usage(cache_dir)
                cache_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
                logger.info(f"üíæ Cache size: {cache_size / (1024**3):.2f} GB")
            except:
                pass

            return 0
        else:
            logger.error("‚ùå Dataset preparation failed!")
            return 1

    except Exception as e:
        logger.error(f"‚ùå Error during dataset preparation: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
